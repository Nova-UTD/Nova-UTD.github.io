<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:8081/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:8081/" rel="alternate" type="text/html" /><updated>2021-08-31T18:13:43-05:00</updated><id>http://localhost:8081/feed.xml</id><title type="html">Voltron</title><subtitle>Driving is hard, but humans make it look easy.  We're &lt;b&gt;UT Dallas's applied autonomous driving project&lt;/b&gt;, and we're making driving look easier than ever.</subtitle><entry><title type="html">Demo 2 Overview: Voltron’s Grand Tour</title><link href="http://localhost:8081/d2-overview" rel="alternate" type="text/html" title="Demo 2 Overview: Voltron’s Grand Tour" /><published>2021-08-31T00:00:00-05:00</published><updated>2021-08-31T00:00:00-05:00</updated><id>http://localhost:8081/Demo-2-overview</id><content type="html" xml:base="http://localhost:8081/d2-overview">&lt;p&gt;Demo 2 is Voltron’s Grand Tour.&lt;/p&gt;

&lt;p&gt;This overview is split into our &lt;strong&gt;success conditions&lt;/strong&gt;, &lt;strong&gt;tasks&lt;/strong&gt;, &lt;strong&gt;timeline&lt;/strong&gt; and &lt;strong&gt;checkpoints&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Success conditions&lt;/strong&gt; outline exactly what Voltron needs to do to finish the Grand Tour.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tasks&lt;/strong&gt; are the challenges we need to complete in order to succeed.&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;timeline&lt;/strong&gt; shows which tasks rely on the completion of other tasks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Checkpoints&lt;/strong&gt; are clear milestones along our journey.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;success-conditions&quot;&gt;Success conditions&lt;/h1&gt;
&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;The trip should be conducted on a light roadway, not within a parking lot or closed course.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;The car should remain within its lane throughout the trip.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;The car should maintain a reasonable speed and acceleration throughout the trip.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;The car should make a complete stop at stop signs, then continue after accounting for other vehicles/pedestrians at the intersection.
    &lt;ul&gt;
      &lt;li&gt;Note: The location of stop signs, along with yield signs, crosswalks, and other static elements, can be provided ahead of time as map annotations. Voltron does not need to detect signage in real time, but it does need to detect and track moving obstacles.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;The car should yield appropriately when entering a roundabout (the roundabout near JSOM and the Visitor Center).&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;The car should stop or adjust speed for obstacles in front of it.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;The car should reduce speed when a pedestrian is significantly near, even if not in front.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Speed, steering, and other key data should be displayed to the human driver throughout the trip.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;tasks&quot;&gt;Tasks&lt;/h1&gt;
&lt;p&gt;The Grand Tour’s tasks have four main components: A new behavior planner, safety improvements, a bigger map, and a new visualization tool.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Revise or rewrite behavior (path) planner
    &lt;ul&gt;
      &lt;li&gt;Should produce appropriate speeds along the path&lt;/li&gt;
      &lt;li&gt;Should stop at stop signs, navigate simple intersections&lt;/li&gt;
      &lt;li&gt;Stop or slow down for nearby obstacles
        &lt;ul&gt;
          &lt;li&gt;Will require an object detector/classifier for pedestrians and vehicles, likely camera and ML-based&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Likely draw from a Moving Object Tracker to predict future location of obstacles&lt;/li&gt;
      &lt;li&gt;Output commands through the steering controller to the Linear Actuators&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Improve platform safety
    &lt;ul&gt;
      &lt;li&gt;Create “heartbeat” signals for all nodes, including for E-stop&lt;/li&gt;
      &lt;li&gt;Write Health Monitor to manage and act on heartbeats&lt;/li&gt;
      &lt;li&gt;Raise flags on specific errors, like a loss in localization confidence or high CPU usage&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Extend operating area
    &lt;ul&gt;
      &lt;li&gt;Create a map of campus, with PCD split into chunks as needed&lt;/li&gt;
      &lt;li&gt;Map should include locations of stop signs, yield signs, and crosswalks (high-traffic pedestrian areas)&lt;/li&gt;
      &lt;li&gt;Write a tool to stitch together our PCD maps into a single clean file&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Visualize key observations and decisions
    &lt;ul&gt;
      &lt;li&gt;Target and current speed/acceleration&lt;/li&gt;
      &lt;li&gt;Target and current steering angle/effort&lt;/li&gt;
      &lt;li&gt;Position of car, signs, and dynamic obstacles on Lanelet map&lt;/li&gt;
      &lt;li&gt;Localization result confidence&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We should focus only on the above goals. Everything else is beyond the scope of Demo 2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/res/d2-overview_map.png&quot; alt=&quot;Grand Tour map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Possible layout of Demo 2 map area (“Grand Tour”), with loop split into eight colored sections. Car will move clockwise along the red route.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;timeline&quot;&gt;Timeline&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;/assets/res/d2-overview_timeline.png&quot; alt=&quot;Timeline&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;checkpoints&quot;&gt;Checkpoints&lt;/h1&gt;
&lt;p&gt;These are major steps along our path to the completion of Demo 2. Checkpoints are &lt;em&gt;not necessarily ordered&lt;/em&gt;, so any checkpoint can be completed before another one. The checkpoints are:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Voltron can localize itself along the entire course&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;B:&lt;/strong&gt; Voltron can generate a correct, sensible path along the entire course (accoutning for position, not speed, so only steering is accounted for)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C:&lt;/strong&gt; Voltron can steer itself along the entire course (essentially Demo 1 on a new course)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;D:&lt;/strong&gt;  Voltron can generate sensible throttle and brake commands, which are then followed by a human driver&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;E:&lt;/strong&gt; Throttle and brake linear actuators can receive manual commands, so that the peddles can be teleoperated via a keyboard&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;F:&lt;/strong&gt; Voltron can control the throttle and brake autonomously, though not correctly&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;G:&lt;/strong&gt; Voltron can control the throttle and brake along the entire course&lt;/p&gt;</content><author><name>Will</name></author><category term="overview" /><summary type="html">Demo 2 is Voltron’s Grand Tour.</summary></entry><entry><title type="html">Rewiring Voltron</title><link href="http://localhost:8081/hardware/2021/08/23/Rewiring-Voltron.html" rel="alternate" type="text/html" title="Rewiring Voltron" /><published>2021-08-23T09:00:18-05:00</published><updated>2021-08-23T09:00:18-05:00</updated><id>http://localhost:8081/hardware/2021/08/23/Rewiring-Voltron</id><content type="html" xml:base="http://localhost:8081/hardware/2021/08/23/Rewiring-Voltron.html">&lt;p&gt;&lt;img src=&quot;/assets/res/2021-08-23-tangle.png&quot; alt=&quot;Untangling colorful lines&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As one final push to get things ready for our fall semester, Voltron underwent a full wiring cleanup. Sketchy soldering jobs were replaced with weatherproof automotive connectors, confusing bunches of wiring spaghetti were replaced with tangle-free conduit, and new components like CAN bus chips were mounted firmly in place.&lt;/p&gt;

&lt;p&gt;Of course clean wiring eliminates frustration, but it’s also much safer than the messy alternative. Solid and well-planned electrical connections are less prone to short circuits, brownouts, and other unwanted behavior. We felt it important to address our wiring immediately in our effort to make Voltron as safe and stable as possible.&lt;/p&gt;

&lt;p&gt;Not much theory here! I’ll only outline a couple methods we used that were helpful.&lt;/p&gt;

&lt;p&gt;First, we chose two common connector formats for all of our connections. First we chose &lt;a href=&quot;https://www.te.com/usa-en/products/connectors/automotive-connectors/intersection/deutsch-dtm-connectors.html?tab=pgp-story&quot;&gt;Deutsch DTM connectors&lt;/a&gt;, which are popular in production cars due to their rugged, weatherproof design. Second we chose the ubiquitous 0.093 inch &lt;a href=&quot;https://www.molex.com/molex/products/family/standard_093&quot;&gt;Molex connectors&lt;/a&gt;, which are found in everything from PC motherboards to pinball machines. They’re not nearly as tough as DTM connectors, but they’re less expensive and complicated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/res/2021-08-23-dtm.jpg&quot; alt=&quot;DTM connector plug and socket&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;DTM connector plug and socket (TE Connectivity)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Second, we wrapped major wire clumps into flexible split conduit. We then routed this conduit around the vehicle’s chassis and attached it to the car’s body using hook-and-loop wrap. Slightly more permanent attachments were made using zip ties.&lt;/p&gt;

&lt;p&gt;Third, we left room for modification and expansion. We used solder sparingly, only in connections that we were sure wouldn’t change. Our flexible solder alternatives ranged from wiring nuts to “quick disconnects” to the DTM and Molex connectors outlined above.&lt;/p&gt;

&lt;p&gt;Below is a schematic of our “secondary” system. This refers to all the components that we’ve added onto the stock Polaris GEM from the factory. You can click the schematic to enlarge it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/static/electrical.html&quot;&gt;&lt;img src=&quot;/assets/res/2021-08-03-schematic.png&quot; alt=&quot;Voltron Secondary System schematic&quot; /&gt;&lt;/a&gt;&lt;/p&gt;</content><author><name>Will Heitman</name></author><category term="hardware" /><summary type="html"></summary></entry><entry><title type="html">Reorganizing Our Stack</title><link href="http://localhost:8081/vde/2021/08/03/Reorganizing-our-stack.html" rel="alternate" type="text/html" title="Reorganizing Our Stack" /><published>2021-08-03T09:00:18-05:00</published><updated>2021-08-03T09:00:18-05:00</updated><id>http://localhost:8081/vde/2021/08/03/Reorganizing-our-stack</id><content type="html" xml:base="http://localhost:8081/vde/2021/08/03/Reorganizing-our-stack.html">&lt;p&gt;We &lt;a href=&quot;/d1-overview&quot;&gt;reached our first milestone&lt;/a&gt; about a month ago, and it felt like we had finished a marathon. What now? It was time to tackle the menace of any major project: &lt;a href=&quot;https://en.wikipedia.org/wiki/Technical_debt&quot;&gt;technical debt&lt;/a&gt;. In our software, this means removing any hacks that we threw together and all the other shortcuts that we took through Demo 1. The best place to start is at the foundation: the organization of the code itself.&lt;/p&gt;

&lt;h1 id=&quot;about-containers&quot;&gt;About containers&lt;/h1&gt;
&lt;p&gt;For most of computing history, our operating systems were responsible for mediating between the unruly applications running on our machines. But the OS doesn’t always work perfectly: Resource-hungry applications can hog our machine’s memory, causing the whole computer to “freeze”. Bad code in a single program can cause the entire OS to “blue screen”. These imperfections are annoying on, say, laptops, but they’re much more serious on autonomous cars zipping down the street.&lt;/p&gt;

&lt;p&gt;We can fix this with a somewhat new approach to software architecture called “containerization”. The idea is simple: Put each program into its own container. Each container mimicks an entire OS, so each program thinks it has the whole machine to itself. Since each container is (to really simplify things) a virtual machine, we can fine tune each container to play nicely with the other containers in our stack.&lt;/p&gt;

&lt;p&gt;Those resource-hungry programs from earlier? We can cap the resources of the container so those memory hogs don’t interfere with the other programs’ performance. Those unstable programs that caused “blue screens” earlier? Although we should always strive to prevent crashes in the first place, the worst-case scenario is now crashing a single container, not the whole machine.&lt;/p&gt;

&lt;p&gt;There are lots of other benefits to containerization: Security, portability, clarity, and so on. Accordingly, this approach to software design has really taken off.&lt;/p&gt;

&lt;p&gt;So how do we take this concept of containers and apply it to our code?&lt;/p&gt;

&lt;h1 id=&quot;our-approach-the-orchestra&quot;&gt;Our approach: The orchestra&lt;/h1&gt;
&lt;p&gt;We use Docker, a highly popular container framework, to wrap each of our &lt;dfn title=&quot;An individual program run in the Robot Operating System framework&quot;&gt;ROS nodes&lt;/dfn&gt; up. Specifically, we create a “frame” image that has all of our tools (Autoware.Auto, ROS2, custom libraries, and so on) pre-installed. For each ROS node, we simply copy the code into our “frame” image, build the ROS node inside the image, and generate a Docker container. Just like that, we’ve nestled a ROS node nicely into a virtual environment.&lt;/p&gt;

&lt;p&gt;Now that all the pieces of our stack are inside their own containers, we use a tool called &lt;a href=&quot;https://docs.docker.com/compose/&quot;&gt;Docker Compose&lt;/a&gt; to run all of our containers in tandem. This is not as simple as executing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run [image]&lt;/code&gt; on all of the pieces. We write a special file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose.yaml&lt;/code&gt; that describes exactly how each piece should be run: What network privileges each container has, what parts it depends on, how much memory it can receive, and so on.&lt;/p&gt;

&lt;p&gt;So now we have a single file that runs all of our containers, and each container has its own piece of our stack. We can now run our entire stack with a single command: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose up&lt;/code&gt;. Done.&lt;/p&gt;

&lt;p&gt;We can think of Compose as the conductor of an orchestra. Each container is a musician, and with the help of the conductor, the musicians create beautiful harmony. When a musician plays too fast, or hits the wrong note, the conductor notices and takes action.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/res/2021-08-03-Reorganizing-our-stack_conductor.png&quot; alt=&quot;The conductor and the orchestra&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This “orchestra” approach is a much safer approach than our previous software approach, where all the musicians just play their own tunes, creating nothing more than noise. Instead of a well-tuned symphony, our software stack resembles kindergarteners with kazoos. When I get behind the wheel of a self-driving car, I’d gladly take the symphony over the kazoos.&lt;/p&gt;

&lt;h1 id=&quot;custom-images&quot;&gt;Custom images&lt;/h1&gt;
&lt;p&gt;I mentioned earlier that each node is put inside a “frame” image that builds the ROS node and runs it. Here are some more details on exactly how our images are structured.&lt;/p&gt;

&lt;p&gt;In Docker, images are created using Dockerfiles. These files are instructions for how a container is built. Remember that a container is (basically) a virtual machine, so Dockerfiles do things like install software, create users, and anything else necessary to set up an environment for our code.&lt;/p&gt;

&lt;p&gt;Dockerfiles start with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FROM&lt;/code&gt; instruction that inherits a parent image. In our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base&lt;/code&gt; container, we start by &lt;a href=&quot;https://github.com/Voltron-UTD/vde/blob/e783ee35f065d9885ad05a2ac9497f33cc47dada/images/base/Dockerfile#L13&quot;&gt;inheriting the official ROS Foxy image&lt;/a&gt;, which in turn inherits the offical Ubuntu image. In a single line, our custom &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base&lt;/code&gt; image now has Ubuntu and ROS installed. The rest of the Dockerfile downloads and builds Autoware, along with its many dependencies.&lt;/p&gt;

&lt;p&gt;Our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pillar&lt;/code&gt; image inherits from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base&lt;/code&gt;, then adds any custom libraries that we write, all of which are stored in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;roslibs&lt;/code&gt; folder in our repo root. At the moment, this is just two custom libraries. We separate this process from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base&lt;/code&gt; because we don’t want to rebuild Autoware every time we add or modify a custom library.&lt;/p&gt;

&lt;p&gt;Finally, our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;frame&lt;/code&gt; image inherits &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pillar&lt;/code&gt;, then copies the code from a ROS package or workspace (specified in our Docker Compose config) and builds it using colcon. When the image is run, it launches our nodes using a ROS launch file.&lt;/p&gt;

&lt;p&gt;All of our custom images are stored in, you guessed it, the &lt;a href=&quot;https://github.com/Voltron-UTD/vde/tree/main/images&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;images&lt;/code&gt; directory&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;using-vde&quot;&gt;Using VDE&lt;/h1&gt;
&lt;p&gt;Our entire stack, including the Docker configs, ROS param files, map data, and of course our program source code, is wrapped in a bundle that we call the &lt;em&gt;Voltron Development Environment&lt;/em&gt;, or VDE (this is a nod to &lt;a href=&quot;https://ade-cli.readthedocs.io/en/latest/index.html&quot;&gt;ADE&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Once our stack has been writte, the only thing left is to run it. As mentioned, this is done using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose up&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Behind the scenes, Docker creates a virtual network interface for each container, then connects the containers to a shared network. This means that although the files and computing resources are virtually isolated in each container, the network is shared among them. This allows ROS to communicate seamlessly across the whole stack, and it also lets us communicate with the stack in the host OS, outside of Docker.&lt;/p&gt;

&lt;h1 id=&quot;future-work&quot;&gt;Future work&lt;/h1&gt;
&lt;p&gt;VDE is still a baby, and it has a lot of growing to do. We plan on adding network rules to make our virtual network more secure. We’d like to add &lt;a href=&quot;https://docs.docker.com/config/containers/start-containers-automatically/#use-a-restart-policy&quot;&gt;restart policies&lt;/a&gt; to each container, which allows us to handle unexpected program errors safely. Of course we’ll add more ROS packages as our project grows. Conspicuously missing from our stack is a user interface, so we’d like to add a web server that serves a web UI.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;You can view and download our code from our &lt;a href=&quot;https://github.com/Voltron-UTD/vde&quot;&gt;GitHub repo&lt;/a&gt;, so go check it out! If you have any suggestions or questions about our work, feel free to &lt;a href=&quot;/#team&quot;&gt;reach out to us&lt;/a&gt;.&lt;/p&gt;</content><author><name>Will Heitman</name></author><category term="vde" /><summary type="html">We reached our first milestone about a month ago, and it felt like we had finished a marathon. What now? It was time to tackle the menace of any major project: technical debt. In our software, this means removing any hacks that we threw together and all the other shortcuts that we took through Demo 1. The best place to start is at the foundation: the organization of the code itself.</summary></entry><entry><title type="html">Simulated Driving</title><link href="http://localhost:8081/simulation/2021/08/03/Simulated-driving.html" rel="alternate" type="text/html" title="Simulated Driving" /><published>2021-08-03T09:00:18-05:00</published><updated>2021-08-03T09:00:18-05:00</updated><id>http://localhost:8081/simulation/2021/08/03/Simulated-driving</id><content type="html" xml:base="http://localhost:8081/simulation/2021/08/03/Simulated-driving.html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/FSZigdNs9aY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;This is just a small post to show our software driving a car in simulation. The process is the same as in real-world use: Provide a location estimate (done manually until we get a GPS sensor) then a goal location. Once the goal location it given, the car immediately steers and accelerates on its own.&lt;/p&gt;

&lt;p&gt;As part of our tests of the &lt;a href=&quot;http://localhost:8080/vde/2021/08/03/Reorganizing-our-stack.html&quot;&gt;updated VDE&lt;/a&gt;, we ran the finished stack in the open-source &lt;a href=&quot;https://www.svlsimulator.com/&quot;&gt;SVL Simulator&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is basically a simulated version of &lt;a href=&quot;http://localhost:8080/d1-overview&quot;&gt;Demo 1&lt;/a&gt;, except that we added a primitive, “bang-bang” controller for throttle and brake. In other words, the entire drive was done hands-free.&lt;/p&gt;

&lt;p&gt;There’s still a whole lot to add! But having the ability to test code in simulation before moving to a real vehicle saves time, and it’s obviously safer too.&lt;/p&gt;

&lt;p&gt;A couple notes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Our map data is misaligned, so the car &lt;em&gt;thinks&lt;/em&gt; that it’s driving perfectly in the lane (look at the blue lane on the right). But in the simulated environment, it’s actually going into the bike lane on the right. Better map alignment would fix this.&lt;/li&gt;
  &lt;li&gt;The car is driving at a fixed target speed of 6 meters per second, or about 13 mph. It can’t go any faster without breaking our localizer.&lt;/li&gt;
  &lt;li&gt;As mentioned, the throttle and brake are controlled using &lt;a href=&quot;https://en.wikipedia.org/wiki/Bang%E2%80%93bang_control&quot;&gt;bang-bang control&lt;/a&gt;, so the car is either pressing the throttle or brake at a fixed strength.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Will Heitman</name></author><category term="simulation" /><summary type="html">This is just a small post to show our software driving a car in simulation. The process is the same as in real-world use: Provide a location estimate (done manually until we get a GPS sensor) then a goal location. Once the goal location it given, the car immediately steers and accelerates on its own.</summary></entry><entry><title type="html">Getting Things Rolling With Demo 1</title><link href="http://localhost:8081/d1-overview" rel="alternate" type="text/html" title="Getting Things Rolling With Demo 1" /><published>2021-07-08T19:00:18-05:00</published><updated>2021-07-08T19:00:18-05:00</updated><id>http://localhost:8081/Getting-things-rolling-with-Demo-1</id><content type="html" xml:base="http://localhost:8081/d1-overview">&lt;p&gt;Getting started with autonomous cars can be scary. How do you start? We started in January with a simple goal: Travel from Point A to Point B. Every other factor would be simplified as much as possible. This led us to the following assumptions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Driving will occur in daytime, in good weather.&lt;/li&gt;
  &lt;li&gt;A human will be behind the wheel at all times, ready to take over.&lt;/li&gt;
  &lt;li&gt;This person will control the throttle and brake.&lt;/li&gt;
  &lt;li&gt;The computer can only move the steering wheel.&lt;/li&gt;
  &lt;li&gt;The computer will have access to processed, pre-recorded map data.&lt;/li&gt;
  &lt;li&gt;The computer will be given its initial location and its goal location before it starts the trip.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also defined a clear success condition, where all of the following are met:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The car should travel from Point A to Point B.&lt;/li&gt;
  &lt;li&gt;Only the computer should apply any control whatsoever to the steering wheel for the duration of the trip.&lt;/li&gt;
  &lt;li&gt;The car should remain in the appropriate lane.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, to meet the above criteria, we outlined five questions that demanded our attention:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;What does the map look like?&lt;/li&gt;
  &lt;li&gt;Where am I on the map?&lt;/li&gt;
  &lt;li&gt;Where is my destination on the map?&lt;/li&gt;
  &lt;li&gt;What is the best path from my location to my destination?&lt;/li&gt;
  &lt;li&gt;How can I translate this path into steering angles?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Breaking our goal down into these five questions made things more clear and approachable.&lt;/p&gt;

&lt;h2 id=&quot;tools-we-used&quot;&gt;Tools we used&lt;/h2&gt;
&lt;h3 id=&quot;software&quot;&gt;Software&lt;/h3&gt;
&lt;p&gt;We’re lucky that a number of autonomous car “platforms” are publicly available for for researchers to use. These platforms offer implementations of popular algorithms for tasks like mapping and control. Platforms include &lt;a href=&quot;https://github.com/commaai/openpilot&quot;&gt;openpilot&lt;/a&gt;, &lt;a href=&quot;https://www.apollo.auto&quot;&gt;apollo&lt;/a&gt;, and a few others. But don’t be fooled like I was– using this software isn’t as easy as downloading a file and running it. Integration is an arduous process, no matter what platform you choose.&lt;/p&gt;

&lt;p&gt;We ultimately decided to use &lt;a href=&quot;https://www.autoware.auto&quot;&gt;Autoware.Auto&lt;/a&gt;. Autoware is a really transparant and easily extensible platform to adopt. It runs off a popular framework called the Robot Operating System, or ROS. Adding features on top of Autoware is as simple as writing our own ROS “node”.&lt;/p&gt;

&lt;h3 id=&quot;electronic-steering-and-can&quot;&gt;Electronic Steering and CAN&lt;/h3&gt;
&lt;p&gt;To steer the vehicle programatically, we used an &lt;abbr title=&quot;electronic power-assisted steering&quot;&gt;EPAS&lt;/abbr&gt; system provided by &lt;a href=&quot;https://www.dcemotorsport.com/Home/EPAS&quot;&gt;DCE Motorsport&lt;/a&gt;. The system includes a powerful motor that fits around the car’s steering column. It also includes a simple computer called an “ECU” that controls the motor.&lt;/p&gt;

&lt;p&gt;To send our commands from our onboard computer to the EPAS ECU, we used a &lt;a href=&quot;https://canable.io/&quot;&gt;CANable Pro&lt;/a&gt;. All modern cars use &lt;abbr title=&quot;Controller Area Networks&quot;&gt;CAN&lt;/abbr&gt; to send data to and from components– signals to turn on the blinkers, honk the horn, and so on. The CANable allows us to connect to our car’s CAN network and send steering commands over it, which the EPAS then receives.&lt;/p&gt;

&lt;h3 id=&quot;on-board-computing&quot;&gt;On-board computing&lt;/h3&gt;
&lt;p&gt;We used an &lt;a href=&quot;https://developer.nvidia.com/embedded/jetson-agx-xavier-developer-kit&quot;&gt;NVIDIA Jetson AGX Xavier&lt;/a&gt; to run our entire stack. We didn’t optimize our code to run efficiently on the Xavier’s GPU, but we still achieved decent performance.&lt;/p&gt;

&lt;p&gt;The Xavier is power efficient, which is an important consideration since our vehicle’s batteries aren’t limitless. Strapping a big GPU workstation to the roof would not have worked.&lt;/p&gt;

&lt;h3 id=&quot;simulation&quot;&gt;Simulation&lt;/h3&gt;
&lt;p&gt;We tested our code in a simulator before testing on the car. We used the &lt;a href=&quot;https://www.svlsimulator.com/&quot;&gt;SVL Simulator&lt;/a&gt;, which can generate highly realistic environments. We ran all of this on a local server.&lt;/p&gt;

&lt;p&gt;A nice thing about our code is that it doesn’t care whether it’s run in the simulator or on the real car. Our software’s only real-time inputs are the Lidar streams, which are provided by the simulator in the same way that they are by the car.&lt;/p&gt;

&lt;h2 id=&quot;1-what-does-the-map-look-like&quot;&gt;1. What does the map look like?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/res/d1-overview_maps.jpg&quot; alt=&quot;Our map has two layers: 3D and labelled&quot; /&gt;
Our map is split into two parts: A 3D map and a labelled map.&lt;/p&gt;

&lt;p&gt;In the 3D map, a driver manually drives through the unmapped area and collects data from the car’s Lidar sensors. The data is then stitched together into a unified map file in a process known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping&quot;&gt;SLAM&lt;/a&gt;. 3D maps are great at describing the texture of an environment. We can line up our 3D map with real-time Lidar streams later on, and that provides us with an accurate location estimate. 3D points combined together are often called “pointclouds”, so we commonly refer to 3D maps as “pointcloud maps” or “PCD maps”.&lt;/p&gt;

&lt;p&gt;The labelled map gives &lt;em&gt;context&lt;/em&gt; to our 3D data. Without it, a street curb is indistiguishable from a speed bump. The vast majority of labelled maps, including ours, are labelled by humans. These maps provide the location of stop signs, crosswalks, and traffic lights. They can store speed limits, road closures, even the locations of soccer fields. Most importantly, the labelled map tells us where lanes are located and how they’re connected. In other words, this map describes the road network. Examples of labelled maps include Google Maps and &lt;a href=&quot;https://www.openstreetmap.org/search?query=University%20of%20Texas%20Dallas#map=16/32.9876/-96.7511&quot;&gt;Open Street Map&lt;/a&gt;. Our maps use a specific format provided by the “Lanelet” library, so we refer to our labelled maps as Lanelet maps.&lt;/p&gt;

&lt;p&gt;When we combine the pointcloud and Lanelet maps together, we create a rich description of our environment. This pointcloud-Lanelet double whammy is often called an “HD” map.&lt;/p&gt;

&lt;h2 id=&quot;2-where-am-i-on-the-map&quot;&gt;2. Where am I on the map?&lt;/h2&gt;
&lt;p&gt;Researchers call this problem “localization”. The most popular localization tool for everyday use is GPS (broadly called GNSS). GPS is not accurate or reliable enough for autonomous cars. Instead, most algorithms take GPS data as a starting point, then refine things with other data (usually LIDAR, stereo cameras).&lt;/p&gt;

&lt;p&gt;We use an algorithm called Normal Distribution Transforms (NDT) to refine an initial location guess into an accurate result using real-time Lidar data and our 3D map. Specifically, we use Autoware.Auto’s implementation of NDT. Find more in &lt;a href=&quot;https://youtu.be/g2YURb-d9vY?t=2532&quot;&gt;this Autoware lecture&lt;/a&gt;. The original NDT paper is &lt;a href=&quot;https://ieeexplore.ieee.org/document/1249285&quot;&gt;available here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The basic steps of NDT are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Divide our prerecorded 3D map into a uniform grid.&lt;/li&gt;
  &lt;li&gt;For each cell in our grid, calculate the average of all the points, along with the covariance. This makes our map much less complex, while still providing general information on how the points are dispersed within the map.&lt;/li&gt;
  &lt;li&gt;Take a real-time Lidar feed from the vehicle and place it onto our map using an initial guess (supplied by a passenger in Demo 1’s case).&lt;/li&gt;
  &lt;li&gt;Move our placed points around until they line up well with our grid of covariances (our processed 3D map).&lt;/li&gt;
  &lt;li&gt;Once our points are aligned, since we know where we are relative to our lidar feed (we’re in the center of the feed) and where our feed is on the map, we then know where we are on the map.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;3-where-is-my-destination-on-the-map&quot;&gt;3. Where is my destination on the map?&lt;/h2&gt;
&lt;p&gt;This one is simple. Since we already have a map from #1, passengers just select a point on the map. This point is stored at our goal pose.&lt;/p&gt;

&lt;h2 id=&quot;4-what-is-the-best-path-from-my-location-to-my-destinaton&quot;&gt;4. What is the best path from my location to my destinaton?&lt;/h2&gt;
&lt;p&gt;Imagine you’re looking for a restaurant that your friend told you about, and you’ve gotten lost. You stop and ask for directions, which are something like, “Go down McKinney Road, then turn right on Rose Parkway, then turn left on…” These general directions, just a sequence of streets to take, is generally good enough for humans.&lt;/p&gt;

&lt;p&gt;In Voltron, we use a &lt;em&gt;route planner&lt;/em&gt; to calculate this sequence of streets using our Lanelet map. It basically generates a sequence like a human would, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[McKinney Road, Rose Parkway...]&lt;/code&gt;. But this isn’t specific enough for computers. We need to refine these general directions into an exact line (curve) that the car follows. This refinement is done by our &lt;em&gt;path planner&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We make the following distinction between “route” and “path”: routes provide general directions while paths are exact curves for the car to drive along. Paths can easily adapt to, say, drive around potholes.&lt;/p&gt;

&lt;h2 id=&quot;5-how-can-i-translate-this-path-into-steering-angles&quot;&gt;5. How can I translate this path into steering angles?&lt;/h2&gt;
&lt;p&gt;At this point we know our current location (from #2) and the path we’d like to follow (from #4). We feed this information into a &lt;em&gt;steering controller&lt;/em&gt; that turns this data into a steering angle.&lt;/p&gt;

&lt;p&gt;Once we have our target steering angle, we feed it along with our current steering angle into a low-level PID controller. This calculates how much torque to apply to the steering wheel (literally how hard to turn the wheel, and it what direction). Finally, we translate this into a command to send to the steering motor. That’s it!&lt;/p&gt;</content><author><name>Will</name></author><category term="overview" /><summary type="html">Getting started with autonomous cars can be scary. How do you start? We started in January with a simple goal: Travel from Point A to Point B. Every other factor would be simplified as much as possible. This led us to the following assumptions: Driving will occur in daytime, in good weather. A human will be behind the wheel at all times, ready to take over. This person will control the throttle and brake. The computer can only move the steering wheel. The computer will have access to processed, pre-recorded map data. The computer will be given its initial location and its goal location before it starts the trip.</summary></entry></feed>