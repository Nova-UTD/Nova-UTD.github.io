<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-08-22T18:28:49-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Nova</title><subtitle>Driving is hard, but humans make it look easy.  We&apos;re &lt;b&gt;UT Dallas&apos;s applied autonomous driving project&lt;/b&gt;, and we&apos;re making driving look easier than ever.</subtitle><entry><title type="html">Two months left!</title><link href="http://localhost:4000/2022/03/03/Two-months-left.html" rel="alternate" type="text/html" title="Two months left!" /><published>2022-03-03T00:00:00-06:00</published><updated>2022-03-03T00:00:00-06:00</updated><id>http://localhost:4000/2022/03/03/Two-months-left</id><content type="html" xml:base="http://localhost:4000/2022/03/03/Two-months-left.html">&lt;p&gt;Hello world! It’s been nearly two months since we’ve updated you on our progress. Things have been crazy here at Nova. We’ve recently migrated our entire simulation environment to &lt;a href=&quot;carlasim.org&quot;&gt;CARLA&lt;/a&gt;. We took part in UTD’s Homecoming Parade with no one in our driver’s seat (see Dylan’s &lt;a href=&quot;https://youtu.be/aLeqj5ZyQQI&quot;&gt;video&lt;/a&gt;). Our brake pedal’s linear actuator suddenly broke, so we’ve had to revamp our pdeal actuation. On and on, everyone on the team is involved in something new, interesting, and challenging. Really challenging.&lt;/p&gt;

&lt;p&gt;To add to all this, we’ve recently crossed the two month mark: Less than two months until Demo 2 has to be wrapped up, recorded, and in the books. Making Demo 2 work is like making a ball out of rubber bands. Anyone who’s tried this will understand that the bands will bend and sag and snap, and you’re dealing with the bending and sagging and snapping of all the bands at once, and they’re twisting and slipping until, all of a sudden, you have a neat, colorful ball in your hands.&lt;/p&gt;

&lt;p&gt;This is just to say that the ball is being made. All of our components, from semantic segmentation to PWM interfaces to map routing, are steadily, twistingly coming together. And in two months the ball will be formed.&lt;/p&gt;

&lt;p&gt;If you’d like to spy on us, you can track our code’s evoluton in real-time &lt;a href=&quot;https://github.com/Nova-UTD/navigator/commits/dev&quot;&gt;on our GitHub&lt;/a&gt;. If you see us testing on campus soon, be sure to wave hello. And be ready to jump out of the way!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/res/2022-03-03-Driverless-HoCo.jpg&quot;&gt;&lt;img src=&quot;/assets/res/2022-03-03-Driverless-HoCo.jpg&quot; alt=&quot;Josh, our software architect, takes shotgun while Kyle (right) walks alongside during the Homecoming Parade. No driver!&quot; /&gt;&lt;/a&gt;
&lt;small&gt;Josh, our software architect, takes shotgun while Kyle (right) walks alongside during the Homecoming Parade. No driver!&lt;/small&gt;&lt;/p&gt;</content><author><name>Will Heitman</name></author><summary type="html">Hello world! It’s been nearly two months since we’ve updated you on our progress. Things have been crazy here at Nova. We’ve recently migrated our entire simulation environment to CARLA. We took part in UTD’s Homecoming Parade with no one in our driver’s seat (see Dylan’s video). Our brake pedal’s linear actuator suddenly broke, so we’ve had to revamp our pdeal actuation. On and on, everyone on the team is involved in something new, interesting, and challenging. Really challenging.</summary></entry><entry><title type="html">Behavior Planning and Controls - Egan Johnson</title><link href="http://localhost:4000/deep/dives/2022/01/11/Behavior-Planning-&-Controls-1.html" rel="alternate" type="text/html" title="Behavior Planning and Controls - Egan Johnson" /><published>2022-01-11T00:00:00-06:00</published><updated>2022-01-11T00:00:00-06:00</updated><id>http://localhost:4000/deep/dives/2022/01/11/Behavior-Planning-&amp;-Controls-1</id><content type="html" xml:base="http://localhost:4000/deep/dives/2022/01/11/Behavior-Planning-&amp;-Controls-1.html">&lt;p&gt;&lt;a href=&quot;/assets/res/headshots/Egan_Johnson.jpg&quot;&gt;&lt;img src=&quot;/assets/res/headshots/Egan_Johnson.jpg&quot; alt=&quot;Headshot of Egan&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hello Comets! From all of us on the Nova team, we hope that you had a restful, restorative, and comfortable break. As we head back to our campus in these uncertain times, we are all hoping that you stay safe, take care of one another, and have a productive start to the year.&lt;/p&gt;

&lt;p&gt;While all of us are coming back to our campus and preparing for our classes, Nova is continuing as usual. We have been working our hardest in order to successfully demonstrate a functional autonomous vehicle in just a few short months. Our team has worked continuously throughout the winter break to get closer to bringing this project to life, and we are so excited to share our work with you soon! In the meantime, we would like to shine a spotlight this week on one of the members of our Behavior Planning and Controls team, Egan Johnson!&lt;/p&gt;

&lt;p&gt;Egan is one of the members of our Behavior Planning &amp;amp; Controls team. Egan is a sophomore studying computer science and hails from the Madison, Wisconsin area. He greatly enjoys running, and was nationally competitive on his high school cross country team.&lt;/p&gt;

&lt;p&gt;For Nova, Egan is something of a veteran, being one of the original members of the Nova team.  Last year, Egan helped work on connecting the data taken from Nova’s mounted LIDAR sensors and converted it into workable data for the algorithms held within Nova’s onboard computer, the Jetson. He was responsible for designing the softwares that converted this information into the “point clouds” that frequent nearly all of Nova’s simulations and programs.&lt;/p&gt;

&lt;p&gt;This year, as part of the Behavior Planning &amp;amp; Controls team, Egan is working extensively on Hail Bopp’s route planner. While the route planner already existed on the vehicle, the older version of the system was very rudimentary, and could only navigate a very simple route in an empty parking lot. The existing system did not include the capability to navigate anything other than bidirectional roads and could not work with lane changes (both of which are, of course, very important to get right when we are trying to get this vehicle onto real roads!). This is now Egan’s project: designing a machine that can plot a path to get from one point to another with the “least cost,” or with the least amount of distance or travel time. This time, the process needs to account for every possible route with all lanes on the road considered!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/res/22-01-11_Egan_Johnson-DataMap.png&quot;&gt;&lt;img src=&quot;/assets/res/22-01-11_Egan_Johnson-DataMap.png&quot; alt=&quot;Datamap with Raw View&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/res/22-01-11_Egan_Johnson-OverviewMap.png&quot;&gt;&lt;img src=&quot;/assets/res/22-01-11_Egan_Johnson-OverviewMap.png&quot; alt=&quot;Datamap with Map View&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is a complex process, involving the softwares in the machine retrieving information from the onboard LIDAR and depth map sensors, along with the calculations from the Perception team’s algorithms, in order to conceptualize all safe, followable paths for the vehicle. These potential paths are updated constantly many times per second in order to have up-to-date routes as new obstacles or variables present themselves around the vehicle. Out of all of these routes, one specific path is chosen by the software for the vehicle to follow, with the safest and most comfortable path being chosen (the code and decisions made on how which of these paths best fit these characteristics will be covered in a Deep-Dive to come!) After this curve is generated and chosen, the path information is passed on to the Controls team, where that data is converted into commands for the wheel, throttle, and brake motors to actuate in specific patterns in order to follow the given path.&lt;/p&gt;

&lt;p&gt;Egan has spent a great deal of time working on Nova. He has a great deal of experience working on the project, and from this, has a piece of advice for students: get involved! From his point of view, Egan says that there are so many small details that go into designing these systems that just are not seen from the outside. There is so much to learn from getting more deeply invested in projects. For Egan, this experience has been great…and for you, the reader, Egan recommends that you do the same! There is so much to gain from being involved, and it can open so many doors to fantastic projects and experience.&lt;/p&gt;

&lt;p&gt;As we continue to work on our project, expect to hear much more from us soon. We cannot wait to show you what we have done, and we wish you all good luck on the start to the new school year!&lt;/p&gt;</content><author><name>Dylan Hiu</name></author><category term="Deep" /><category term="Dives" /><summary type="html"></summary></entry><entry><title type="html">Firmware Specialist - Avery Bainbridge</title><link href="http://localhost:4000/deep/dives/2021/11/12/Firmware-Specialist-Deep-Dive.html" rel="alternate" type="text/html" title="Firmware Specialist - Avery Bainbridge" /><published>2021-11-12T00:00:00-06:00</published><updated>2021-11-12T00:00:00-06:00</updated><id>http://localhost:4000/deep/dives/2021/11/12/Firmware-Specialist-Deep-Dive</id><content type="html" xml:base="http://localhost:4000/deep/dives/2021/11/12/Firmware-Specialist-Deep-Dive.html">&lt;p&gt;&lt;a href=&quot;/assets/res/headshots/avery_bainbridge.jpg&quot;&gt;&lt;img src=&quot;/assets/res/headshots/avery_bainbridge.jpg&quot; alt=&quot;Headshot of Avery&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Avery Bainbridge, Nova’s Firmware Specialist.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Last week, we introduced you to Nova’s Technical Artist, Phu Le. You got to hear a little more about the work that is going into Demo 2…but our 3D models are only the start of what’s cooking. There is still so much else we have going on at Nova: this week, you will be learning more about another critical member of our team, Avery Bainbridge, who serves as Nova’s Firmware Specialist!&lt;/p&gt;

&lt;p&gt;Avery is in charge of taking the rest of the team’s code and putting it all together on the “Jetson.” The Jetson is the name of Hail Bopp’s onboard computer. It is a small version of any other computer, with CPU processors, graphics processors, and a huge assortment of other bits and bobs. The powerful GPU’s in the Jetson are not just for games; in fact, they are in charge of a very a large portion of the thinking that our vehicle does! Avery’s job is to ensure that all of the code created by our team works together seamlessly with the Jetson, along with ensuring the data sent to the Jetson from our onboard sensors interfaces correctly with our systems.&lt;/p&gt;

&lt;p&gt;Avery hails from Boise, Idaho, and brings a wealth of experience to this position. In high school, Avery was a member of the FIRST Robotics Competitions (FRC) for four years, and spent a huge amount of time working on microcontroller projects on his own time. He learned the tricks of the trade through countless hours of work in robotics and vision processing: in his own words, “Quite a bit of what you expect when you program is that you have an operating system underneath you. They helped you out. You have to learn sort of how to do without it when you’re doing microcontroller programming.”&lt;/p&gt;

&lt;p&gt;In the past few weeks, Avery has been working on integrating the GPS receiver from Hail Bopp into workable code for the rest of the team. The written code from our team talks to our GPS modules and needs to receive back workable position and velocity data to interface with the rest of the systems in the vehicle.  Interfacing this code is not trivial: there are an enormous amount of issues to solve before any of this data can be usable for us. Differences in communication protocols, changing the GPS’ positioning function from an ellipsoid to a reference geoid, properly passing the latitude, longitude, and altitude data to the localization stack to be processed…All of this information is needed to give Hail Bopp a “ground truth” to determine its exact position in space. Avery is the man responsible for making it all happen.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/res/21-11-12_Avery_Bainbridge_working.png&quot;&gt;&lt;img src=&quot;/assets/res/21-11-12_Avery_Bainbridge_working.png&quot; alt=&quot;Image of Avery working in the Lab&quot; /&gt;&lt;/a&gt;
&lt;small&gt;Avery at work in the Nova lab.&lt;small&gt;&lt;/small&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Avery’s work has become a bedrock for the project, and has been essential to getting us up and running this semester. In recognition of his excellent work, Avery is the first recipient of Nova’s weekly award…our rubber chicken! Avery will hold onto this prize for a grand total of one week before it is passed on to next week’s Nova Member of the Week.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/res/21-11-12-Avery_Bainbridge_chicken.jpg&quot;&gt;&lt;img src=&quot;/assets/res/21-11-12-Avery_Bainbridge_chicken.jpg&quot; alt=&quot;Image of Avery with our rubber chicken&quot; /&gt;&lt;/a&gt;
  &lt;small&gt;Avery being recognized with Nova’s extra-special rubber chicken.&lt;small&gt;&lt;/small&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Avery wants students to remember that in technology, underneath all of the high-level functionality and capability they possess that seem like magic, there is an enormous amount of work that goes into the essential functions of the product. Underneath the higher level abstractions of the product, “It’s not magic, and that there are real people such as myself.” He wants to remind students that underneath all of the code and functions, to quote Isaac Newton, “Don’t forget that you’re working on the shoulders of giants.” Nova constantly relies on Avery to keep everything on the right track, and we cannot do our work without him!&lt;/p&gt;</content><author><name>Dylan Hiu</name></author><category term="Deep" /><category term="Dives" /><summary type="html"></summary></entry><entry><title type="html">Data Visualization &amp;amp; Interface - Raghav Pillai</title><link href="http://localhost:4000/deep/dives/2021/11/12/Data-Visualization-&-Interface.html" rel="alternate" type="text/html" title="Data Visualization &amp;amp; Interface - Raghav Pillai" /><published>2021-11-12T00:00:00-06:00</published><updated>2021-11-12T00:00:00-06:00</updated><id>http://localhost:4000/deep/dives/2021/11/12/Data-Visualization-&amp;-Interface</id><content type="html" xml:base="http://localhost:4000/deep/dives/2021/11/12/Data-Visualization-&amp;-Interface.html">&lt;p&gt;&lt;a href=&quot;/assets/res/headshots/raghav_pillai.jpg&quot;&gt;&lt;img src=&quot;/assets/res/headshots/raghav_pillai.jpg&quot; alt=&quot;Headshot of Raghav&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In our previous post, we debuted our weekly tradition of awarding our prized Rubber Chicken to a member of the Nova team who we feel deserves special recognition for their contributions to our goals. This week, we recognize Raghav Pillai, our head of Data Visualization &amp;amp; Interface design!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/res/21-11-12_Raghav_Chicken.png&quot;&gt;&lt;img src=&quot;/assets/res/21-11-12_Raghav_Chicken.png&quot; alt=&quot;Raghav with Chicken&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Raghav’s recognition this week emphasizes his dedication to Nova and the hard work he’s put into the project. Aside from Nova, Raghav is an active student around campus, being involved with multiple other projects. A junior in computer science, Raghav is involved extracurriculars both at an undergraduate level and with other graduate students. In fact, Raghav recently place second in Hack UTD in the category of Investment Strategies for Goldman Sachs! He has spent a great deal of time working in freelance and full-stack code development, and his wealth of experience elevates Nova to an entirely different level.&lt;/p&gt;

&lt;p&gt;Raghav’s role in Nova as the head of Data Visualization combines the work of multiple different members of the team. His aim is to take the raw data from Hail Bopp’s sensors along with the data from our GPS and positional equipment mounted on the vehicle and convert them into a visual representation of what the vehicle “sees” as it travels through space. This work is a complicated process that involves collaboration with almost every group within Nova. For example, Data from the Perception Team is needed in order to process and identify the surroundings of the vehicle. 3D renders from our Technical Artist are needed to be integrated with this data in order to have models for the identified objects. Finally, Raghav’s work helps collect all of this information together to accurately represent the space around the vehicle. This data will eventually be displayed on a monitor mounted on the interior of the vehicle, allowing the operator and passengers to have an idea of what the car is detecting around it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/res/21-11-12_Overhead_Visualization.png&quot;&gt;&lt;img src=&quot;/assets/res/21-11-12_Overhead_Visualization.png&quot; alt=&quot;Overhead Visualization&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This type of operation is critical for multiple reasons. The first is that from an engineering perspective, we need to be able to know precisely what the car detects around it. Without this data, creating a safe path for the vehicle to follow is impossible. It also prevents us from being able to create functional emergency stop algorithms. After all, if the vehicle is not able to detect obstacles in its path, then to the car, there is no reason to stop. This, of course, is not ideal for us: we would prefer if Hail Bopp did not ram itself through a bush in its path that it failed to detect.&lt;/p&gt;

&lt;p&gt;The other reason that Raghav’s work is so important is that it greatly increases the feeling of safety and security for passengers. Imagine if, as a passenger in a self-driving vehicle, the car began to drive, brake, and steer without any indication of what the car is aware of. This can be extremely unsettling for passengers. However, the digital visualization of what is around the vehicle can give customers a sense of security. Knowing that the car can also see the obstacles and surroundings in the same way that the passengers do can be reassuring for those unfamiliar with autonomous vehicles. In fact, since the car can receive data from all around the vehicle and display an intuitive top-down view of its surroundings, the visualization can give passengers a sense that the vehicle is entirely in control in its environment. You can a closer look at the beginning of this visualization below!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/res/21-11-12_Overhead_Visualization_2.png&quot;&gt;&lt;img src=&quot;/assets/res/21-11-12_Overhead_Visualization_2.png&quot; alt=&quot;Overhead Visualization 2&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Raghav’s work accomplishes these aims, but he also recognizes the contributions of that of the rest of the team to his work. To Raghav, “What Nova is more than anything is that it’s kind of a symbol to show what you can do with a team with people who are motivated. It’s a symbol of what we can do together if we work together as a team.” He also offers students who have big dreams some encouragement: “This project is a testament to all the people out there who don’t know where to get started. You just need to get started to make something.”&lt;/p&gt;</content><author><name>Dylan Hiu</name></author><category term="Deep" /><category term="Dives" /><summary type="html"></summary></entry><entry><title type="html">Technical Artist Deep Dive - Phu Le</title><link href="http://localhost:4000/deep/dives/2021/11/04/Technical-Artist-Deep-Dive.html" rel="alternate" type="text/html" title="Technical Artist Deep Dive - Phu Le" /><published>2021-11-04T00:00:00-05:00</published><updated>2021-11-04T00:00:00-05:00</updated><id>http://localhost:4000/deep/dives/2021/11/04/Technical-Artist-Deep-Dive</id><content type="html" xml:base="http://localhost:4000/deep/dives/2021/11/04/Technical-Artist-Deep-Dive.html">&lt;p&gt;&lt;a href=&quot;/assets/res/headshots/phu_le.jpg&quot;&gt;&lt;img src=&quot;/assets/res/headshots/phu_le.jpg&quot; alt=&quot;Headshot of Phu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Phu Le, Nova’s Technical Artist.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;UT Dallas’ autonomous vehicle project is underway! This Fall, 15 undergraduate students have come together to continue the process of creating a fully functional self-driving vehicle, entirely from scratch. This is no small feat, and has the Nova team working on an enormous array of issues around the clock in order to make this dream a reality. Nova’s aim this year is Demo 2: an ambitious project with an aim to have the project’s vehicle, Hail Bopp (a retrofitted Polaris Gem) safely complete an entirely autonomous loop around the streets surrounding the perimeter of UT Dallas alongside the traffic, skateboarders, and Tobors UTD students are familiar with.&lt;/p&gt;

&lt;p&gt;Before Nova’s testbed vehicle, Hail Bopp, can make its way to the streets of UTD, each member of the team will have to sort their way through an array of problems. Nearly all of these problems will be solved through simulations in virtual environments. Every scenario needs to be tested thousands of times before Hail Bopp can make its way out of the garage on its own. However, in order to test the vehicle, we need a digital environment to test it in…&lt;/p&gt;

&lt;p&gt;This week, we will be taking a deep-dive on Phu Le, Nova’s Technical Artist. A sophomore in Computer Science, Phu has spent years learning graphic design and 3D modeling…entirely on his own. Through Youtube, online tutorials, and an enormous amount of practice, Phu has honed his skills to the point that he can fly through Blender and Photoshop with no issue. These skills were good enough that Phu has already spent time interning for a professional photography firm as a graphic designer. A member of the Student Game Developer Association on campus, Phu has a great deal of experience and talent in regards to environment and object design, animation, and 3D modeling.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/res/2021-11-04-Phu_Le_Portfolio1.png&quot;&gt;&lt;img src=&quot;/assets/res/2021-11-04-Phu_Le_Portfolio1.png&quot; alt=&quot;Image from Phu&apos;s online portfolio&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;/assets/res/2021-11-04-Phu_Le-Portfolio2.png&quot;&gt;&lt;img src=&quot;/assets/res/2021-11-04-Phu_Le-Portfolio2.png&quot; alt=&quot;Second from Phu&apos;s online portfolio&quot; /&gt;&lt;/a&gt;
&lt;small&gt;Images from Phu’s previous work in 3D modeling and game design.&lt;small&gt;&lt;/small&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;For Nova, Phu has been tasked with creating a 3D, detailed render of Hail Bopp from top-to-bottom. Polaris, the manufacturer of the Gem vehicle used as the basis of the project, does not have any scan or model of the vehicle available, leaving it up to Phu to create them for us from scratch! This model will be used in all future simulations for the entirety of the project, and will allow the Nova team to approximate their simulations against reality. In addition, Phu will also be in charge of environment design for these virtual environments as well, with some of them being the streets, buildings, and trees around our campus. In fact, Phu plans on modeling the entirety of UT Dallas in the future! He has also in charge of designing Nova’s logos and marketing materials.&lt;/p&gt;

&lt;p&gt;These extremely detailed environments make it possible for the developers to stress-test, challenge, and generally put Hail Bopp through hell without putting anyone (or the vehicle!) in harms way. This process is very similar to how Tesla uses their own in-house environment to test their vehicles before production. Since Phu can change these environments at will, the team can put the vehicle through any situation they can possibly think of. His work is incredibly important to ensuring that by Demo 2, Hail Bopp will be able to face all sorts of situations and can navigate any environment it may encounter while navigating UTD.&lt;/p&gt;

&lt;p&gt;Phu’s work makes it possible for the rest of the team to simulate their designs in environments very similar to what we will face in Demo 2. In future updates, keep an eye out for his renders in Nova’s testing simulations!&lt;/p&gt;</content><author><name>Dylan Hiu</name></author><category term="Deep" /><category term="Dives" /><summary type="html"></summary></entry><entry><title type="html">Hello, new members! (And goodbye to an old one)</title><link href="http://localhost:4000/team/2021/10/12/Welcoming-new-members.html" rel="alternate" type="text/html" title="Hello, new members! (And goodbye to an old one)" /><published>2021-10-12T16:00:18-05:00</published><updated>2021-10-12T16:00:18-05:00</updated><id>http://localhost:4000/team/2021/10/12/Welcoming-new-members</id><content type="html" xml:base="http://localhost:4000/team/2021/10/12/Welcoming-new-members.html">&lt;p&gt;&lt;a href=&quot;/assets/res/2021-09-08-Group_photo.jpg&quot;&gt;&lt;img src=&quot;/assets/res/2021-09-08-Group_photo.jpg&quot; alt=&quot;Our full team, including new members&quot; /&gt;&lt;/a&gt;
&lt;small&gt;From left to right: Phu Le, technical artist; Avery Bainbridge, firmware specialist; Justin Ruths, faculty advisor; Cristian Cruz, behavior planning and controls (BPC); Joshua Williams, software architect; Quinn Loach, education and outreach; Nikhik Narvekar, BPC; Jim Moore, BPC; Kyle Zeng, perception; Raghav Pillai, web interface; Will Heitman, team lead; Connor Scally, simulation; Egan Johnson, BPC; Ragib Arnab, perception; Dylan Hiu, PR. Click to enlarge.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;I’m proud to welcome nine new members to Nova, bringing our full team to fifteen talented undergrads. One year ago, Nova (called Voltron then) was just one undergrad and a faculty advisor. Six additional undergrad students carried us through &lt;a href=&quot;/d1-overview&quot;&gt;Demo 1&lt;/a&gt;. As we begin our push toward &lt;a href=&quot;/d2-overview&quot;&gt;Demo 2&lt;/a&gt;, I know that our new members will be a big help.&lt;/p&gt;

&lt;p&gt;Our new members consist of algorithm development specialists, a technical artist, a PR director, a simulation engineer, a web application developer, and an education and outreach director. Each new member brings their own unique skillset, and I’m excited to see where we’ll go together. Autonomous driving is a crazy path to travel on, and it’s always better travel with a group.&lt;/p&gt;

&lt;p&gt;While I welcome our new members, I’d also like give a big thank you and farewell to Grace Zhu. Grace was instrumental in developing our map data for Demo 1. Many of the mapping techniques that we rely on today were discovered by Grace, and she has made a lasting contribution to our project. Grace has recently moved back to UT Austin, where she studies as a CS undergrad.&lt;/p&gt;

&lt;p&gt;Take a look at our &lt;a href=&quot;/#our-team&quot;&gt;updated membership section&lt;/a&gt;, where you can click on each of our members to read their story (thanks to Raghav for building this). One more welcome to Nikhil, Jim, Ragib, Avery, Connor, Dylan, Quinn, Phu, and Raghav!&lt;/p&gt;</content><author><name>Will Heitman</name></author><category term="team" /><summary type="html">From left to right: Phu Le, technical artist; Avery Bainbridge, firmware specialist; Justin Ruths, faculty advisor; Cristian Cruz, behavior planning and controls (BPC); Joshua Williams, software architect; Quinn Loach, education and outreach; Nikhik Narvekar, BPC; Jim Moore, BPC; Kyle Zeng, perception; Raghav Pillai, web interface; Will Heitman, team lead; Connor Scally, simulation; Egan Johnson, BPC; Ragib Arnab, perception; Dylan Hiu, PR. Click to enlarge.</summary></entry><entry><title type="html">Rewiring Voltron</title><link href="http://localhost:4000/hardware/2021/08/23/Rewiring-Voltron.html" rel="alternate" type="text/html" title="Rewiring Voltron" /><published>2021-08-23T09:00:18-05:00</published><updated>2021-08-23T09:00:18-05:00</updated><id>http://localhost:4000/hardware/2021/08/23/Rewiring-Voltron</id><content type="html" xml:base="http://localhost:4000/hardware/2021/08/23/Rewiring-Voltron.html">&lt;p&gt;&lt;img src=&quot;/assets/res/2021-08-23-tangle.png&quot; alt=&quot;Untangling colorful lines&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As one final push to get things ready for our fall semester, Voltron underwent a full wiring cleanup. Sketchy soldering jobs were replaced with weatherproof automotive connectors, confusing bunches of wiring spaghetti were replaced with tangle-free conduit, and new components like CAN bus chips were mounted firmly in place.&lt;/p&gt;

&lt;p&gt;Of course clean wiring eliminates frustration, but it’s also much safer than the messy alternative. Solid and well-planned electrical connections are less prone to short circuits, brownouts, and other unwanted behavior. We felt it important to address our wiring immediately in our effort to make Voltron as safe and stable as possible.&lt;/p&gt;

&lt;p&gt;Not much theory here! I’ll only outline a couple methods we used that were helpful.&lt;/p&gt;

&lt;p&gt;First, we chose two common connector formats for all of our connections. First we chose &lt;a href=&quot;https://www.te.com/usa-en/products/connectors/automotive-connectors/intersection/deutsch-dtm-connectors.html?tab=pgp-story&quot;&gt;Deutsch DTM connectors&lt;/a&gt;, which are popular in production cars due to their rugged, weatherproof design. Second we chose the ubiquitous 0.093 inch &lt;a href=&quot;https://www.molex.com/molex/products/family/standard_093&quot;&gt;Molex connectors&lt;/a&gt;, which are found in everything from PC motherboards to pinball machines. They’re not nearly as tough as DTM connectors, but they’re less expensive and complicated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/res/2021-08-23-dtm.jpg&quot; alt=&quot;DTM connector plug and socket&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;DTM connector plug and socket (TE Connectivity)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Second, we wrapped major wire clumps into flexible split conduit. We then routed this conduit around the vehicle’s chassis and attached it to the car’s body using hook-and-loop wrap. Slightly more permanent attachments were made using zip ties.&lt;/p&gt;

&lt;p&gt;Third, we left room for modification and expansion. We used solder sparingly, only in connections that we were sure wouldn’t change. Our flexible solder alternatives ranged from wiring nuts to “quick disconnects” to the DTM and Molex connectors outlined above.&lt;/p&gt;

&lt;p&gt;Below is a schematic of our “secondary” system. This refers to all the components that we’ve added onto the stock Polaris GEM from the factory. You can click the schematic to enlarge it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/static/electrical.html&quot;&gt;&lt;img src=&quot;/assets/res/2021-08-03-schematic.png&quot; alt=&quot;Voltron Secondary System schematic&quot; /&gt;&lt;/a&gt;&lt;/p&gt;</content><author><name>Will Heitman</name></author><category term="hardware" /><summary type="html"></summary></entry><entry><title type="html">Reorganizing Our Stack</title><link href="http://localhost:4000/vde/2021/08/03/Reorganizing-our-stack.html" rel="alternate" type="text/html" title="Reorganizing Our Stack" /><published>2021-08-03T09:00:18-05:00</published><updated>2021-08-03T09:00:18-05:00</updated><id>http://localhost:4000/vde/2021/08/03/Reorganizing-our-stack</id><content type="html" xml:base="http://localhost:4000/vde/2021/08/03/Reorganizing-our-stack.html">&lt;p&gt;We &lt;a href=&quot;/d1-overview&quot;&gt;reached our first milestone&lt;/a&gt; about a month ago, and it felt like we had finished a marathon. What now? It was time to tackle the menace of any major project: &lt;a href=&quot;https://en.wikipedia.org/wiki/Technical_debt&quot;&gt;technical debt&lt;/a&gt;. In our software, this means removing any hacks that we threw together and all the other shortcuts that we took through Demo 1. The best place to start is at the foundation: the organization of the code itself.&lt;/p&gt;

&lt;h1 id=&quot;about-containers&quot;&gt;About containers&lt;/h1&gt;
&lt;p&gt;For most of computing history, our operating systems were responsible for mediating between the unruly applications running on our machines. But the OS doesn’t always work perfectly: Resource-hungry applications can hog our machine’s memory, causing the whole computer to “freeze”. Bad code in a single program can cause the entire OS to “blue screen”. These imperfections are annoying on, say, laptops, but they’re much more serious on autonomous cars zipping down the street.&lt;/p&gt;

&lt;p&gt;We can fix this with a somewhat new approach to software architecture called “containerization”. The idea is simple: Put each program into its own container. Each container mimicks an entire OS, so each program thinks it has the whole machine to itself. Since each container is (to really simplify things) a virtual machine, we can fine tune each container to play nicely with the other containers in our stack.&lt;/p&gt;

&lt;p&gt;Those resource-hungry programs from earlier? We can cap the resources of the container so those memory hogs don’t interfere with the other programs’ performance. Those unstable programs that caused “blue screens” earlier? Although we should always strive to prevent crashes in the first place, the worst-case scenario is now crashing a single container, not the whole machine.&lt;/p&gt;

&lt;p&gt;There are lots of other benefits to containerization: Security, portability, clarity, and so on. Accordingly, this approach to software design has really taken off.&lt;/p&gt;

&lt;p&gt;So how do we take this concept of containers and apply it to our code?&lt;/p&gt;

&lt;h1 id=&quot;our-approach-the-orchestra&quot;&gt;Our approach: The orchestra&lt;/h1&gt;
&lt;p&gt;We use Docker, a highly popular container framework, to wrap each of our &lt;dfn title=&quot;An individual program run in the Robot Operating System framework&quot;&gt;ROS nodes&lt;/dfn&gt; up. Specifically, we create a “frame” image that has all of our tools (Autoware.Auto, ROS2, custom libraries, and so on) pre-installed. For each ROS node, we simply copy the code into our “frame” image, build the ROS node inside the image, and generate a Docker container. Just like that, we’ve nestled a ROS node nicely into a virtual environment.&lt;/p&gt;

&lt;p&gt;Now that all the pieces of our stack are inside their own containers, we use a tool called &lt;a href=&quot;https://docs.docker.com/compose/&quot;&gt;Docker Compose&lt;/a&gt; to run all of our containers in tandem. This is not as simple as executing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run [image]&lt;/code&gt; on all of the pieces. We write a special file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose.yaml&lt;/code&gt; that describes exactly how each piece should be run: What network privileges each container has, what parts it depends on, how much memory it can receive, and so on.&lt;/p&gt;

&lt;p&gt;So now we have a single file that runs all of our containers, and each container has its own piece of our stack. We can now run our entire stack with a single command: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose up&lt;/code&gt;. Done.&lt;/p&gt;

&lt;p&gt;We can think of Compose as the conductor of an orchestra. Each container is a musician, and with the help of the conductor, the musicians create beautiful harmony. When a musician plays too fast, or hits the wrong note, the conductor notices and takes action.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/res/2021-08-03-Reorganizing-our-stack_conductor.png&quot; alt=&quot;The conductor and the orchestra&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This “orchestra” approach is a much safer approach than our previous software approach, where all the musicians just play their own tunes, creating nothing more than noise. Instead of a well-tuned symphony, our software stack resembles kindergarteners with kazoos. When I get behind the wheel of a self-driving car, I’d gladly take the symphony over the kazoos.&lt;/p&gt;

&lt;h1 id=&quot;custom-images&quot;&gt;Custom images&lt;/h1&gt;
&lt;p&gt;I mentioned earlier that each node is put inside a “frame” image that builds the ROS node and runs it. Here are some more details on exactly how our images are structured.&lt;/p&gt;

&lt;p&gt;In Docker, images are created using Dockerfiles. These files are instructions for how a container is built. Remember that a container is (basically) a virtual machine, so Dockerfiles do things like install software, create users, and anything else necessary to set up an environment for our code.&lt;/p&gt;

&lt;p&gt;Dockerfiles start with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FROM&lt;/code&gt; instruction that inherits a parent image. In our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base&lt;/code&gt; container, we start by &lt;a href=&quot;https://github.com/Nova-UTD/navigator/blob/e783ee35f065d9885ad05a2ac9497f33cc47dada/images/base/Dockerfile#L13&quot;&gt;inheriting the official ROS Foxy image&lt;/a&gt;, which in turn inherits the offical Ubuntu image. In a single line, our custom &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base&lt;/code&gt; image now has Ubuntu and ROS installed. The rest of the Dockerfile downloads and builds Autoware, along with its many dependencies.&lt;/p&gt;

&lt;p&gt;Our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pillar&lt;/code&gt; image inherits from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base&lt;/code&gt;, then adds any custom libraries that we write, all of which are stored in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;roslibs&lt;/code&gt; folder in our repo root. At the moment, this is just two custom libraries. We separate this process from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;base&lt;/code&gt; because we don’t want to rebuild Autoware every time we add or modify a custom library.&lt;/p&gt;

&lt;p&gt;Finally, our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;frame&lt;/code&gt; image inherits &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pillar&lt;/code&gt;, then copies the code from a ROS package or workspace (specified in our Docker Compose config) and builds it using colcon. When the image is run, it launches our nodes using a ROS launch file.&lt;/p&gt;

&lt;p&gt;All of our custom images are stored in, you guessed it, the &lt;a href=&quot;https://github.com/Voltron-UTD/vde/tree/main/images&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;images&lt;/code&gt; directory&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;using-vde&quot;&gt;Using VDE&lt;/h1&gt;
&lt;p&gt;Our entire stack, including the Docker configs, ROS param files, map data, and of course our program source code, is wrapped in a bundle that we call the &lt;em&gt;Voltron Development Environment&lt;/em&gt;, or VDE (this is a nod to &lt;a href=&quot;https://ade-cli.readthedocs.io/en/latest/index.html&quot;&gt;ADE&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Once our stack has been writte, the only thing left is to run it. As mentioned, this is done using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose up&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Behind the scenes, Docker creates a virtual network interface for each container, then connects the containers to a shared network. This means that although the files and computing resources are virtually isolated in each container, the network is shared among them. This allows ROS to communicate seamlessly across the whole stack, and it also lets us communicate with the stack in the host OS, outside of Docker.&lt;/p&gt;

&lt;h1 id=&quot;future-work&quot;&gt;Future work&lt;/h1&gt;
&lt;p&gt;VDE is still a baby, and it has a lot of growing to do. We plan on adding network rules to make our virtual network more secure. We’d like to add &lt;a href=&quot;https://docs.docker.com/config/containers/start-containers-automatically/#use-a-restart-policy&quot;&gt;restart policies&lt;/a&gt; to each container, which allows us to handle unexpected program errors safely. Of course we’ll add more ROS packages as our project grows. Conspicuously missing from our stack is a user interface, so we’d like to add a web server that serves a web UI.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;You can view and download our code from our &lt;a href=&quot;https://github.com/Nova-UTD/navigator&quot;&gt;GitHub repo&lt;/a&gt;, so go check it out! If you have any suggestions or questions about our work, feel free to &lt;a href=&quot;/#team&quot;&gt;reach out to us&lt;/a&gt;.&lt;/p&gt;</content><author><name>Will Heitman</name></author><category term="vde" /><summary type="html">We reached our first milestone about a month ago, and it felt like we had finished a marathon. What now? It was time to tackle the menace of any major project: technical debt. In our software, this means removing any hacks that we threw together and all the other shortcuts that we took through Demo 1. The best place to start is at the foundation: the organization of the code itself.</summary></entry><entry><title type="html">Simulated Driving</title><link href="http://localhost:4000/simulation/2021/08/03/Simulated-driving.html" rel="alternate" type="text/html" title="Simulated Driving" /><published>2021-08-03T09:00:18-05:00</published><updated>2021-08-03T09:00:18-05:00</updated><id>http://localhost:4000/simulation/2021/08/03/Simulated-driving</id><content type="html" xml:base="http://localhost:4000/simulation/2021/08/03/Simulated-driving.html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/FSZigdNs9aY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;This is just a small post to show our software driving a car in simulation. The process is the same as in real-world use: Provide a location estimate (done manually until we get a GPS sensor) then a goal location. Once the goal location it given, the car immediately steers and accelerates on its own.&lt;/p&gt;

&lt;p&gt;As part of our tests of the &lt;a href=&quot;http://localhost:8080/vde/2021/08/03/Reorganizing-our-stack.html&quot;&gt;updated VDE&lt;/a&gt;, we ran the finished stack in the open-source &lt;a href=&quot;https://www.svlsimulator.com/&quot;&gt;SVL Simulator&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is basically a simulated version of &lt;a href=&quot;http://localhost:8080/d1-overview&quot;&gt;Demo 1&lt;/a&gt;, except that we added a primitive, “bang-bang” controller for throttle and brake. In other words, the entire drive was done hands-free.&lt;/p&gt;

&lt;p&gt;There’s still a whole lot to add! But having the ability to test code in simulation before moving to a real vehicle saves time, and it’s obviously safer too.&lt;/p&gt;

&lt;p&gt;A couple notes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Our map data is misaligned, so the car &lt;em&gt;thinks&lt;/em&gt; that it’s driving perfectly in the lane (look at the blue lane on the right). But in the simulated environment, it’s actually going into the bike lane on the right. Better map alignment would fix this.&lt;/li&gt;
  &lt;li&gt;The car is driving at a fixed target speed of 6 meters per second, or about 13 mph. It can’t go any faster without breaking our localizer.&lt;/li&gt;
  &lt;li&gt;As mentioned, the throttle and brake are controlled using &lt;a href=&quot;https://en.wikipedia.org/wiki/Bang%E2%80%93bang_control&quot;&gt;bang-bang control&lt;/a&gt;, so the car is either pressing the throttle or brake at a fixed strength.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Will Heitman</name></author><category term="simulation" /><summary type="html">This is just a small post to show our software driving a car in simulation. The process is the same as in real-world use: Provide a location estimate (done manually until we get a GPS sensor) then a goal location. Once the goal location it given, the car immediately steers and accelerates on its own.</summary></entry><entry><title type="html">Getting Things Rolling With Demo 1</title><link href="http://localhost:4000/d1-overview" rel="alternate" type="text/html" title="Getting Things Rolling With Demo 1" /><published>2021-07-08T19:00:18-05:00</published><updated>2021-07-08T19:00:18-05:00</updated><id>http://localhost:4000/Getting-things-rolling-with-Demo-1</id><content type="html" xml:base="http://localhost:4000/d1-overview">&lt;p&gt;Getting started with autonomous cars can be scary. How do you start? We started in January with a simple goal: Travel from Point A to Point B. Every other factor would be simplified as much as possible. This led us to the following assumptions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Driving will occur in daytime, in good weather.&lt;/li&gt;
  &lt;li&gt;A human will be behind the wheel at all times, ready to take over.&lt;/li&gt;
  &lt;li&gt;This person will control the throttle and brake.&lt;/li&gt;
  &lt;li&gt;The computer can only move the steering wheel.&lt;/li&gt;
  &lt;li&gt;The computer will have access to processed, pre-recorded map data.&lt;/li&gt;
  &lt;li&gt;The computer will be given its initial location and its goal location before it starts the trip.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also defined a clear success condition, where all of the following are met:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The car should travel from Point A to Point B.&lt;/li&gt;
  &lt;li&gt;Only the computer should apply any control whatsoever to the steering wheel for the duration of the trip.&lt;/li&gt;
  &lt;li&gt;The car should remain in the appropriate lane.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, to meet the above criteria, we outlined five questions that demanded our attention:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;What does the map look like?&lt;/li&gt;
  &lt;li&gt;Where am I on the map?&lt;/li&gt;
  &lt;li&gt;Where is my destination on the map?&lt;/li&gt;
  &lt;li&gt;What is the best path from my location to my destination?&lt;/li&gt;
  &lt;li&gt;How can I translate this path into steering angles?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Breaking our goal down into these five questions made things more clear and approachable.&lt;/p&gt;

&lt;h2 id=&quot;tools-we-used&quot;&gt;Tools we used&lt;/h2&gt;
&lt;h3 id=&quot;software&quot;&gt;Software&lt;/h3&gt;
&lt;p&gt;We’re lucky that a number of autonomous car “platforms” are publicly available for for researchers to use. These platforms offer implementations of popular algorithms for tasks like mapping and control. Platforms include &lt;a href=&quot;https://github.com/commaai/openpilot&quot;&gt;openpilot&lt;/a&gt;, &lt;a href=&quot;https://www.apollo.auto&quot;&gt;apollo&lt;/a&gt;, and a few others. But don’t be fooled like I was– using this software isn’t as easy as downloading a file and running it. Integration is an arduous process, no matter what platform you choose.&lt;/p&gt;

&lt;p&gt;We ultimately decided to use &lt;a href=&quot;https://www.autoware.auto&quot;&gt;Autoware.Auto&lt;/a&gt;. Autoware is a really transparant and easily extensible platform to adopt. It runs off a popular framework called the Robot Operating System, or ROS. Adding features on top of Autoware is as simple as writing our own ROS “node”.&lt;/p&gt;

&lt;h3 id=&quot;electronic-steering-and-can&quot;&gt;Electronic Steering and CAN&lt;/h3&gt;
&lt;p&gt;To steer the vehicle programatically, we used an &lt;abbr title=&quot;electronic power-assisted steering&quot;&gt;EPAS&lt;/abbr&gt; system provided by &lt;a href=&quot;https://www.dcemotorsport.com/Home/EPAS&quot;&gt;DCE Motorsport&lt;/a&gt;. The system includes a powerful motor that fits around the car’s steering column. It also includes a simple computer called an “ECU” that controls the motor.&lt;/p&gt;

&lt;p&gt;To send our commands from our onboard computer to the EPAS ECU, we used a &lt;a href=&quot;https://canable.io/&quot;&gt;CANable Pro&lt;/a&gt;. All modern cars use &lt;abbr title=&quot;Controller Area Networks&quot;&gt;CAN&lt;/abbr&gt; to send data to and from components– signals to turn on the blinkers, honk the horn, and so on. The CANable allows us to connect to our car’s CAN network and send steering commands over it, which the EPAS then receives.&lt;/p&gt;

&lt;h3 id=&quot;on-board-computing&quot;&gt;On-board computing&lt;/h3&gt;
&lt;p&gt;We used an &lt;a href=&quot;https://developer.nvidia.com/embedded/jetson-agx-xavier-developer-kit&quot;&gt;NVIDIA Jetson AGX Xavier&lt;/a&gt; to run our entire stack. We didn’t optimize our code to run efficiently on the Xavier’s GPU, but we still achieved decent performance.&lt;/p&gt;

&lt;p&gt;The Xavier is power efficient, which is an important consideration since our vehicle’s batteries aren’t limitless. Strapping a big GPU workstation to the roof would not have worked.&lt;/p&gt;

&lt;h3 id=&quot;simulation&quot;&gt;Simulation&lt;/h3&gt;
&lt;p&gt;We tested our code in a simulator before testing on the car. We used the &lt;a href=&quot;https://www.svlsimulator.com/&quot;&gt;SVL Simulator&lt;/a&gt;, which can generate highly realistic environments. We ran all of this on a local server.&lt;/p&gt;

&lt;p&gt;A nice thing about our code is that it doesn’t care whether it’s run in the simulator or on the real car. Our software’s only real-time inputs are the Lidar streams, which are provided by the simulator in the same way that they are by the car.&lt;/p&gt;

&lt;h2 id=&quot;1-what-does-the-map-look-like&quot;&gt;1. What does the map look like?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/res/d1-overview_maps.jpg&quot; alt=&quot;Our map has two layers: 3D and labelled&quot; /&gt;
Our map is split into two parts: A 3D map and a labelled map.&lt;/p&gt;

&lt;p&gt;In the 3D map, a driver manually drives through the unmapped area and collects data from the car’s Lidar sensors. The data is then stitched together into a unified map file in a process known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping&quot;&gt;SLAM&lt;/a&gt;. 3D maps are great at describing the texture of an environment. We can line up our 3D map with real-time Lidar streams later on, and that provides us with an accurate location estimate. 3D points combined together are often called “pointclouds”, so we commonly refer to 3D maps as “pointcloud maps” or “PCD maps”.&lt;/p&gt;

&lt;p&gt;The labelled map gives &lt;em&gt;context&lt;/em&gt; to our 3D data. Without it, a street curb is indistiguishable from a speed bump. The vast majority of labelled maps, including ours, are labelled by humans. These maps provide the location of stop signs, crosswalks, and traffic lights. They can store speed limits, road closures, even the locations of soccer fields. Most importantly, the labelled map tells us where lanes are located and how they’re connected. In other words, this map describes the road network. Examples of labelled maps include Google Maps and &lt;a href=&quot;https://www.openstreetmap.org/search?query=University%20of%20Texas%20Dallas#map=16/32.9876/-96.7511&quot;&gt;Open Street Map&lt;/a&gt;. Our maps use a specific format provided by the “Lanelet” library, so we refer to our labelled maps as Lanelet maps.&lt;/p&gt;

&lt;p&gt;When we combine the pointcloud and Lanelet maps together, we create a rich description of our environment. This pointcloud-Lanelet double whammy is often called an “HD” map.&lt;/p&gt;

&lt;h2 id=&quot;2-where-am-i-on-the-map&quot;&gt;2. Where am I on the map?&lt;/h2&gt;
&lt;p&gt;Researchers call this problem “localization”. The most popular localization tool for everyday use is GPS (broadly called GNSS). GPS is not accurate or reliable enough for autonomous cars. Instead, most algorithms take GPS data as a starting point, then refine things with other data (usually LIDAR, stereo cameras).&lt;/p&gt;

&lt;p&gt;We use an algorithm called Normal Distribution Transforms (NDT) to refine an initial location guess into an accurate result using real-time Lidar data and our 3D map. Specifically, we use Autoware.Auto’s implementation of NDT. Find more in &lt;a href=&quot;https://youtu.be/g2YURb-d9vY?t=2532&quot;&gt;this Autoware lecture&lt;/a&gt;. The original NDT paper is &lt;a href=&quot;https://ieeexplore.ieee.org/document/1249285&quot;&gt;available here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The basic steps of NDT are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Divide our prerecorded 3D map into a uniform grid.&lt;/li&gt;
  &lt;li&gt;For each cell in our grid, calculate the average of all the points, along with the covariance. This makes our map much less complex, while still providing general information on how the points are dispersed within the map.&lt;/li&gt;
  &lt;li&gt;Take a real-time Lidar feed from the vehicle and place it onto our map using an initial guess (supplied by a passenger in Demo 1’s case).&lt;/li&gt;
  &lt;li&gt;Move our placed points around until they line up well with our grid of covariances (our processed 3D map).&lt;/li&gt;
  &lt;li&gt;Once our points are aligned, since we know where we are relative to our lidar feed (we’re in the center of the feed) and where our feed is on the map, we then know where we are on the map.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;3-where-is-my-destination-on-the-map&quot;&gt;3. Where is my destination on the map?&lt;/h2&gt;
&lt;p&gt;This one is simple. Since we already have a map from #1, passengers just select a point on the map. This point is stored at our goal pose.&lt;/p&gt;

&lt;h2 id=&quot;4-what-is-the-best-path-from-my-location-to-my-destinaton&quot;&gt;4. What is the best path from my location to my destinaton?&lt;/h2&gt;
&lt;p&gt;Imagine you’re looking for a restaurant that your friend told you about, and you’ve gotten lost. You stop and ask for directions, which are something like, “Go down McKinney Road, then turn right on Rose Parkway, then turn left on…” These general directions, just a sequence of streets to take, is generally good enough for humans.&lt;/p&gt;

&lt;p&gt;In Voltron, we use a &lt;em&gt;route planner&lt;/em&gt; to calculate this sequence of streets using our Lanelet map. It basically generates a sequence like a human would, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[McKinney Road, Rose Parkway...]&lt;/code&gt;. But this isn’t specific enough for computers. We need to refine these general directions into an exact line (curve) that the car follows. This refinement is done by our &lt;em&gt;path planner&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We make the following distinction between “route” and “path”: routes provide general directions while paths are exact curves for the car to drive along. Paths can easily adapt to, say, drive around potholes.&lt;/p&gt;

&lt;h2 id=&quot;5-how-can-i-translate-this-path-into-steering-angles&quot;&gt;5. How can I translate this path into steering angles?&lt;/h2&gt;
&lt;p&gt;At this point we know our current location (from #2) and the path we’d like to follow (from #4). We feed this information into a &lt;em&gt;steering controller&lt;/em&gt; that turns this data into a steering angle.&lt;/p&gt;

&lt;p&gt;Once we have our target steering angle, we feed it along with our current steering angle into a low-level PID controller. This calculates how much torque to apply to the steering wheel (literally how hard to turn the wheel, and it what direction). Finally, we translate this into a command to send to the steering motor. That’s it!&lt;/p&gt;</content><author><name>Will</name></author><category term="overview" /><summary type="html">Getting started with autonomous cars can be scary. How do you start? We started in January with a simple goal: Travel from Point A to Point B. Every other factor would be simplified as much as possible. This led us to the following assumptions: Driving will occur in daytime, in good weather. A human will be behind the wheel at all times, ready to take over. This person will control the throttle and brake. The computer can only move the steering wheel. The computer will have access to processed, pre-recorded map data. The computer will be given its initial location and its goal location before it starts the trip.</summary></entry></feed>